import os
import csv
import shutil
import logging
import networkx as nx
from dotenv import load_dotenv
from concurrent.futures import ProcessPoolExecutor
from functools import partial

# .env 파일 로드
load_dotenv(dotenv_path='../.env')

# 로깅 설정
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def read_executed_identifiers(file_path):
    try:
        with open(file_path, 'r') as f:
            return [line.strip() for line in f.readlines()[1:]]  # Skip header
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        return []
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {str(e)}")
        return []

def process_file(process_func, src_dir, dest_dir, hash_value):
    try:
        src_file = os.path.join(src_dir, f"{hash_value}.csv")
        dest_file = os.path.join(dest_dir, f"{hash_value}.csv")
        if os.path.exists(src_file):
            process_func(src_file, dest_file)
            return True
        else:
            logger.warning(f"Source file not found: {src_file}")
            return False
    except Exception as e:
        logger.error(f"Error processing {hash_value}: {str(e)}")
        return False

def process_api_calls(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)    # Skip header
        api_calls_seq = [row[0] for row in reader][:int(os.getenv('MAX_FEATURE_LENGTH'))]  # Limit to 1000 API calls
        writer.writerows([[call] for call in api_calls_seq])

def process_file_system(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)  # Skip header
        for i, row in enumerate(reader):
            if i >= int(os.getenv('MAX_FEATURE_LENGTH')):
                break
            writer.writerow(row)

def process_registry(src_file, dest_file):
    process_file_system(src_file, dest_file)  # Same process as file system

def process_opcodes(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)  # Skip header
        opcodes = [row[0] for row in reader][:int(os.getenv('MAX_FEATURE_LENGTH'))]
        writer.writerows([[opcode] for opcode in opcodes])

def process_strings(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)  # Skip header
        strings = [row[0] for row in reader][:int(os.getenv('MAX_FEATURE_LENGTH'))]  # Limit to 1000 strings
        writer.writerows([[string] for string in strings])

def process_import_table(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)  # Skip header
        imports = [f"{row[0]}\\{row[1]}" for row in reader][:int(os.getenv('MAX_FEATURE_LENGTH'))]  # Limit to 1000 imports
        writer.writerows([[imp] for imp in imports])

def process_feature(feature_name, process_func, src_dir, dest_dir, executed_identifiers):
    logger.info(f"Pre-processing {feature_name}")
    os.makedirs(dest_dir, exist_ok=True)

    with ProcessPoolExecutor(max_workers=4) as executor:
        process_partial = partial(process_file, process_func, src_dir, dest_dir)
        results = list(executor.map(process_partial, executed_identifiers))

    processed_count = sum(results)
    logger.info(f"Processed {processed_count} out of {len(executed_identifiers)} files for {feature_name}")

def main(dynamic_features_dir, static_features_dir, output_dir):
    logger.info("Starting data processing...")
    executed_identifiers_file = os.path.join(dynamic_features_dir, "executed_identifiers.csv")
    executed_identifiers = read_executed_identifiers(executed_identifiers_file)
    logger.info(f"Found {len(executed_identifiers)} executed hashes")

    features = [
        ("DYNAMIC FEATURES : API calls", process_api_calls, os.path.join(dynamic_features_dir, "dynamic-api_calls"),
         os.path.join(output_dir, "dynamic", "api_calls")),
        ("DYNAMIC FEATURES : File System", process_file_system,
         os.path.join(dynamic_features_dir, "dynamic-file_changes"),
         os.path.join(output_dir, "dynamic", "file_system")),
        (
        "DYNAMIC FEATURES : Registry", process_registry, os.path.join(dynamic_features_dir, "dynamic-registry_changes"),
        os.path.join(output_dir, "dynamic", "registry")),
        ("STATIC FEATURES : opcodes", process_opcodes, os.path.join(static_features_dir, "opcodes"),
         os.path.join(output_dir, "static", "opcodes")),
        ("STATIC FEATURES : strings", process_strings, os.path.join(static_features_dir, "strings"),
         os.path.join(output_dir, "static", "strings")),
        ("STATIC FEATURES : Import Table", process_import_table, os.path.join(static_features_dir, "dlls"),
         os.path.join(output_dir, "static", "import_table")),
    ]

    for feature_name, process_func, src_dir, dest_dir in features:
        process_feature(feature_name, process_func, src_dir, dest_dir, executed_identifiers)

    logger.info("Data processing completed.")