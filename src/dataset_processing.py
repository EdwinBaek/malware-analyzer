import os
import csv
import shutil
import logging
import networkx as nx
from concurrent.futures import ProcessPoolExecutor
from functools import partial

# 로깅 설정
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# 설정 파일 (config.py로 분리 가능)
CONFIG = {
    'BASE_DIR': os.path.dirname(os.path.abspath(__file__)),
    'DATASET_DIR': os.path.join(os.path.dirname(os.path.abspath(__file__)), "dataset"),
    'MAX_WORKERS': 4  # 병렬 처리를 위한 최대 worker 수
}

CONFIG['DYNAMIC_FEATURES_DIR'] = os.path.join(CONFIG['DATASET_DIR'], "dynamic_features")
CONFIG['STATIC_FEATURES_DIR'] = os.path.join(CONFIG['DATASET_DIR'], "static_features")
CONFIG['OUTPUT_DIR'] = os.path.join(CONFIG['DATASET_DIR'], "features")


def read_executed_identifiers(file_path):
    try:
        with open(file_path, 'r') as f:
            return [line.strip() for line in f.readlines()[1:]]  # Skip header
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        return []
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {str(e)}")
        return []


def process_file(process_func, src_dir, dest_dir, hash_value):
    try:
        src_file = os.path.join(src_dir, f"{hash_value}.csv")
        dest_file = os.path.join(dest_dir, f"{hash_value}.csv")
        if os.path.exists(src_file):
            process_func(src_file, dest_file)
            return True
        else:
            logger.warning(f"Source file not found: {src_file}")
            return False
    except Exception as e:
        logger.error(f"Error processing {hash_value}: {str(e)}")
        return False


def process_api_calls(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)    # Skip header
        writer.writerows(reader)


def process_file_system(src_file, dest_file):
    G = nx.DiGraph()
    with open(src_file, 'r') as src:
        reader = csv.reader(src)
        next(reader)  # Skip header
        for row in reader:
            if len(row) >= 4:
                api, status, path = row[1], row[2], row[3]
                G.add_edge(api, path, status=status)
    nx.write_edgelist(G, dest_file, data=True)


def process_registry(src_file, dest_file):
    process_file_system(src_file, dest_file)  # Same process as file system


def process_opcode(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)  # Skip header
        writer.writerows([row[0]] for row in reader)  # Only write opcode


def process_strings(src_file, dest_file):
    shutil.copy(src_file, dest_file)
    # Remove header
    with open(dest_file, 'r') as f:
        lines = f.readlines()
    with open(dest_file, 'w') as f:
        f.writelines(lines[1:])


def process_import_table(src_file, dest_file):
    with open(src_file, 'r') as src, open(dest_file, 'w', newline='') as dest:
        reader = csv.reader(src)
        writer = csv.writer(dest)
        next(reader)  # Skip header
        writer.writerows([f"{row[0]}\\{row[1]}"] for row in reader)


def process_feature(feature_name, process_func, src_dir, dest_dir, executed_identifiers):
    logger.info(f"Pre-processing {feature_name}")
    os.makedirs(dest_dir, exist_ok=True)

    with ProcessPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
        process_partial = partial(process_file, process_func, src_dir, dest_dir)
        results = list(executor.map(process_partial, executed_identifiers))

    processed_count = sum(results)
    logger.info(f"Processed {processed_count} out of {len(executed_identifiers)} files for {feature_name}")


def main(dynamic_features_dir, static_features_dir, output_dir):
    logger.info("Starting data processing...")
    executed_identifiers_file = os.path.join(dynamic_features_dir, "executed_identifiers.csv")
    executed_identifiers = read_executed_identifiers(executed_identifiers_file)
    logger.info(f"Found {len(executed_identifiers)} executed hashes")

    features = [
        ("DYNAMIC FEATURES : API calls", process_api_calls, os.path.join(dynamic_features_dir, "dynamic-api_calls"),
         os.path.join(output_dir, "dynamic", "api_calls")),
        ("DYNAMIC FEATURES : File System", process_file_system,
         os.path.join(dynamic_features_dir, "dynamic-file_changes"),
         os.path.join(output_dir, "dynamic", "file_system")),
        (
        "DYNAMIC FEATURES : Registry", process_registry, os.path.join(dynamic_features_dir, "dynamic-registry_changes"),
        os.path.join(output_dir, "dynamic", "registry")),
        ("STATIC FEATURES : opcode", process_opcode, os.path.join(static_features_dir, "opcodes"),
         os.path.join(output_dir, "static", "opcode")),
        ("STATIC FEATURES : strings", process_strings, os.path.join(static_features_dir, "strings"),
         os.path.join(output_dir, "static", "strings")),
        ("STATIC FEATURES : Import Table", process_import_table, os.path.join(static_features_dir, "dlls"),
         os.path.join(output_dir, "static", "import_table")),
    ]

    for feature_name, process_func, src_dir, dest_dir in features:
        process_feature(feature_name, process_func, src_dir, dest_dir, executed_identifiers)

    logger.info("Data processing completed.")